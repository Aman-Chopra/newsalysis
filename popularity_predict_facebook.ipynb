{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "news_final = pd.read_csv(\"/Users/sumanur/Desktop/ml/octeight.csv\") \n",
    "news_final = news_final.dropna()\n",
    "news_final = news_final.drop(columns=['SentimentTitle', 'SentimentHeadline', 'IDLink'])\n",
    "news_final = news_final.drop(columns=['GooglePlus'])\n",
    "news_final = news_final.drop(columns=['LinkedIn'])\n",
    "news_final = news_final.drop(columns=['Topics_lda'])\n",
    "news_final = news_final.drop(columns=['Topics_lda_title'])\n",
    "\n",
    "\n",
    "# Sentiment Analysis \n",
    "headlines = list(news_final['Headline'])\n",
    "titles = list(news_final['Title'])\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Adds headline sentiment to the dataframe\n",
    "headline_sentiment = []\n",
    "for h in headlines:\n",
    "    hs = sia.polarity_scores(h)\n",
    "    headline_sentiment.append(hs)\n",
    "compound_headline_sentiments = []\n",
    "for hs in headline_sentiment:\n",
    "    compound_headline_sentiments.append(hs['compound'])\n",
    "    \n",
    "compound_headline_sentiments = pd.Series(compound_headline_sentiments)\n",
    "news_final['HeadlineSentiment'] = compound_headline_sentiments.values\n",
    "\n",
    "# Adds title sentiment to the dataframe\n",
    "title_sentiment = []\n",
    "for t in titles:\n",
    "    ts = sia.polarity_scores(t)\n",
    "    title_sentiment.append(ts)\n",
    "compound_title_sentiments = []\n",
    "for ts in title_sentiment:\n",
    "    compound_title_sentiments.append(ts['compound'])\n",
    "    \n",
    "compound_title_sentiments = pd.Series(compound_title_sentiments)\n",
    "news_final['TitleSentiment'] = compound_title_sentiments.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove time stamp and add 'Hour of Day' column and 'Day of Week' column\n",
    "import datetime\n",
    "news_dates = news_final['PublishDate']\n",
    "\n",
    "hour_of_day = []\n",
    "for date in news_dates:\n",
    "    news_date =  datetime.datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "    news_hour = news_date.hour\n",
    "    hour_of_day.append(news_hour)\n",
    "news_final['HourOfDay'] = hour_of_day\n",
    "news_final['HourOfDay'] = news_final['HourOfDay']//3\n",
    "\n",
    "day_of_week = []\n",
    "for date in news_dates:\n",
    "    news_date =  datetime.datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "    news_day = news_date.weekday()\n",
    "    day_of_week.append(news_day)\n",
    "news_final['DayOfWeek'] = day_of_week\n",
    "\n",
    "news_final = news_final.drop(columns=['PublishDate'])\n",
    "news_final = news_final.drop(columns=['Title'])\n",
    "news_final = news_final.drop(columns=['Headline'])\n",
    "news_final = news_final.drop(columns=['Source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 to 3 topic labels\n",
    "topic_labels = []\n",
    "for e in news_final['Topic']:\n",
    "    if e == 'economy':\n",
    "        topic_labels.append(0)\n",
    "    if e == 'obama':\n",
    "        topic_labels.append(1)\n",
    "    if e == 'palestine':\n",
    "        topic_labels.append(2)\n",
    "    if e == 'microsoft':\n",
    "        topic_labels.append(3)\n",
    "\n",
    "news_final['TopicLabels'] = topic_labels\n",
    "news_final = news_final.drop(columns=['Topic'])\n",
    "\n",
    "# Scaling\n",
    "news_final['HourOfDay'] = news_final['HourOfDay']/7\n",
    "news_final['DayOfWeek'] = news_final['DayOfWeek']/6\n",
    "news_final['TopicLabels'] = news_final['TopicLabels']/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add K means column\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "X = np.array(news_final['Facebook'])\n",
    "X = X.reshape(-1,1)\n",
    "\n",
    "kk = 2\n",
    "kmeans = KMeans(n_clusters=kk, random_state=0, max_iter=100, algorithm=\"full\", ).fit(X)\n",
    "news_final['FBPopMean'] = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster based oversampling\n",
    "m0 = news_final[news_final.FBPopMean == 0]\n",
    "m1 = news_final[news_final.FBPopMean == 1]\n",
    "\n",
    "m0_final = m0\n",
    "m1_final = m1\n",
    "\n",
    "while len(m1_final) < len(m0):\n",
    "    m1_final = m1_final.append(m1)\n",
    "\n",
    "news_final_mega = m0_final.append(m1_final)\n",
    "news_final = news_final_mega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Prepare input for ANN\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "Y = enc.fit_transform(np.array(news_final['FBPopMean']).reshape(-1,1))\n",
    "X = np.array(news_final.drop(columns=['Facebook','FBPopMean']))\n",
    "\n",
    "# Define ANN\n",
    "model = Sequential([\n",
    "    Dense(8, input_shape=(5,)),Activation('linear'),\n",
    "    Dense(6),Activation('sigmoid'),\n",
    "    Dense(kk),Activation('softmax'),\n",
    "])\n",
    "model.compile(optimizer='nadam', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "184953/184953 [==============================] - 11s 58us/step - loss: 0.5394 - acc: 0.7416\n",
      "Epoch 2/20\n",
      "184953/184953 [==============================] - 11s 58us/step - loss: 0.4824 - acc: 0.7958\n",
      "Epoch 3/20\n",
      "184953/184953 [==============================] - 10s 54us/step - loss: 0.4812 - acc: 0.7959\n",
      "Epoch 4/20\n",
      "184953/184953 [==============================] - 9s 51us/step - loss: 0.4800 - acc: 0.7958\n",
      "Epoch 5/20\n",
      "184953/184953 [==============================] - 9s 51us/step - loss: 0.4777 - acc: 0.7959\n",
      "Epoch 6/20\n",
      "184953/184953 [==============================] - 10s 52us/step - loss: 0.4754 - acc: 0.7958\n",
      "Epoch 7/20\n",
      "184953/184953 [==============================] - 10s 52us/step - loss: 0.4739 - acc: 0.7959\n",
      "Epoch 8/20\n",
      "184953/184953 [==============================] - 10s 54us/step - loss: 0.4723 - acc: 0.7970\n",
      "Epoch 9/20\n",
      "184953/184953 [==============================] - 10s 52us/step - loss: 0.4710 - acc: 0.7978\n",
      "Epoch 10/20\n",
      "184953/184953 [==============================] - 10s 54us/step - loss: 0.4700 - acc: 0.7987\n",
      "Epoch 11/20\n",
      "184953/184953 [==============================] - 10s 56us/step - loss: 0.4696 - acc: 0.7988\n",
      "Epoch 12/20\n",
      "184953/184953 [==============================] - 10s 52us/step - loss: 0.4689 - acc: 0.7990\n",
      "Epoch 13/20\n",
      "184953/184953 [==============================] - 10s 53us/step - loss: 0.4687 - acc: 0.7992 4s - loss: 0.4700 - - ETA: 3s - loss: - ETA: 2s - loss: 0.4 - ETA: 1s - loss: 0.4692 - acc: 0. - ETA\n",
      "Epoch 14/20\n",
      "184953/184953 [==============================] - 10s 54us/step - loss: 0.4681 - acc: 0.7993\n",
      "Epoch 15/20\n",
      "184953/184953 [==============================] - 11s 58us/step - loss: 0.4677 - acc: 0.7989\n",
      "Epoch 16/20\n",
      "184953/184953 [==============================] - 14s 74us/step - loss: 0.4675 - acc: 0.7987\n",
      "Epoch 17/20\n",
      "184953/184953 [==============================] - 13s 73us/step - loss: 0.4671 - acc: 0.7985\n",
      "Epoch 18/20\n",
      "184953/184953 [==============================] - 14s 74us/step - loss: 0.4670 - acc: 0.7984\n",
      "Epoch 19/20\n",
      "184953/184953 [==============================] - 13s 72us/step - loss: 0.4668 - acc: 0.7981\n",
      "Epoch 20/20\n",
      "184953/184953 [==============================] - 14s 77us/step - loss: 0.4666 - acc: 0.7983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x112f05588>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "model.fit(X,Y, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy = \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6048357447176625"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "xx =X\n",
    "yy = news_final['FBPopMean']\n",
    "yy = np.array(yy)\n",
    "xx\n",
    "from sklearn.model_selection import train_test_split\n",
    "xx_train, xx_test, yy_train, yy_test = train_test_split(xx, yy, test_size=0.25, random_state=0)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression(max_iter=1000)\n",
    "logisticRegr.fit(xx_train, yy_train)\n",
    "print(\"Logistic Regression Accuracy = \")\n",
    "logisticRegr.score(xx_test, yy_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation Scores\n",
      "-0.11170412735609\n",
      "-0.14991247218875992\n",
      "-0.17682638004548462\n",
      "0.08600980469356881\n",
      "0.041156951030780084\n"
     ]
    }
   ],
   "source": [
    "# Correlation Scores\n",
    "print(\"Correlation Scores\")\n",
    "print(news_final['FBPopMean'].corr(news_final['TopicLabels']))\n",
    "print(news_final['FBPopMean'].corr(news_final['HeadlineSentiment']))\n",
    "print(news_final['FBPopMean'].corr(news_final['TitleSentiment']))\n",
    "print(news_final['FBPopMean'].corr(news_final['HourOfDay']))\n",
    "print(news_final['FBPopMean'].corr(news_final['DayOfWeek']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
