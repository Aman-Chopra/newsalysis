{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "news_final = pd.read_csv(\"/Users/sumanur/Desktop/ml/octeight.csv\") \n",
    "news_final = news_final.dropna()\n",
    "news_final = news_final.drop(columns=['SentimentTitle', 'SentimentHeadline', 'IDLink'])\n",
    "news_final = news_final.drop(columns=['LinkedIn'])\n",
    "news_final = news_final.drop(columns=['Facebook'])\n",
    "news_final = news_final.drop(columns=['Topics_lda'])\n",
    "news_final = news_final.drop(columns=['Topics_lda_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis \n",
    "headlines = list(news_final['Headline'])\n",
    "titles = list(news_final['Title'])\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Adds headline sentiment to the dataframe\n",
    "headline_sentiment = []\n",
    "for h in headlines:\n",
    "    hs = sia.polarity_scores(h)\n",
    "    headline_sentiment.append(hs)\n",
    "compound_headline_sentiments = []\n",
    "for hs in headline_sentiment:\n",
    "    compound_headline_sentiments.append(hs['compound'])\n",
    "    \n",
    "compound_headline_sentiments = pd.Series(compound_headline_sentiments)\n",
    "news_final['HeadlineSentiment'] = compound_headline_sentiments.values\n",
    "\n",
    "# Adds title sentiment to the dataframe\n",
    "title_sentiment = []\n",
    "for t in titles:\n",
    "    ts = sia.polarity_scores(t)\n",
    "    title_sentiment.append(ts)\n",
    "compound_title_sentiments = []\n",
    "for ts in title_sentiment:\n",
    "    compound_title_sentiments.append(ts['compound'])\n",
    "    \n",
    "compound_title_sentiments = pd.Series(compound_title_sentiments)\n",
    "news_final['TitleSentiment'] = compound_title_sentiments.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove time stamp and add 'Hour of Day' column and 'Day of Week' column\n",
    "import datetime\n",
    "news_dates = news_final['PublishDate']\n",
    "\n",
    "hour_of_day = []\n",
    "for date in news_dates:\n",
    "    news_date =  datetime.datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "    news_hour = news_date.hour\n",
    "    hour_of_day.append(news_hour)\n",
    "news_final['HourOfDay'] = hour_of_day\n",
    "news_final['HourOfDay'] = news_final['HourOfDay']//3\n",
    "\n",
    "day_of_week = []\n",
    "for date in news_dates:\n",
    "    news_date =  datetime.datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "    news_day = news_date.weekday()\n",
    "    day_of_week.append(news_day)\n",
    "news_final['DayOfWeek'] = day_of_week\n",
    "\n",
    "# Discard\n",
    "news_final = news_final.drop(columns=['PublishDate'])\n",
    "news_final = news_final.drop(columns=['Title'])\n",
    "news_final = news_final.drop(columns=['Headline'])\n",
    "news_final = news_final.drop(columns=['Source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 to 3 topic labels\n",
    "topic_labels = []\n",
    "for e in news_final['Topic']:\n",
    "    if e == 'economy':\n",
    "        topic_labels.append(0)\n",
    "    if e == 'obama':\n",
    "        topic_labels.append(1)\n",
    "    if e == 'palestine':\n",
    "        topic_labels.append(2)\n",
    "    if e == 'microsoft':\n",
    "        topic_labels.append(3)\n",
    "\n",
    "news_final['TopicLabels'] = topic_labels\n",
    "news_final = news_final.drop(columns=['Topic'])\n",
    "\n",
    "# Scaling\n",
    "news_final['HourOfDay'] = news_final['HourOfDay']/7\n",
    "news_final['DayOfWeek'] = news_final['DayOfWeek']/6\n",
    "news_final['TopicLabels'] = news_final['TopicLabels']/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Add K means column\n",
    "X = np.array(news_final['GooglePlus'])\n",
    "X = X.reshape(-1,1)\n",
    "kk = 2\n",
    "kmeans = KMeans(n_clusters=kk, random_state=0, max_iter=100, algorithm=\"full\", ).fit(X)\n",
    "news_final['FBPopMean'] = kmeans.predict(X)\n",
    "\n",
    "# Cluster based oversampling\n",
    "m0 = news_final[news_final.FBPopMean == 0]\n",
    "m1 = news_final[news_final.FBPopMean == 1]\n",
    "\n",
    "m0_final = m0\n",
    "m1_final = m1\n",
    "\n",
    "while len(m1_final) < len(m0):\n",
    "    m1_final = m1_final.append(m1)\n",
    "\n",
    "news_final_mega = m0_final.append(m1_final)\n",
    "news_final = news_final_mega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Prepare input for ANN\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "Y = enc.fit_transform(np.array(news_final['FBPopMean']).reshape(-1,1))\n",
    "X = np.array(news_final.drop(columns=['GooglePlus','FBPopMean']))\n",
    "\n",
    "# ANN model specifications\n",
    "model = Sequential([\n",
    "    Dense(8, input_shape=(5,)),Activation('sigmoid'),\n",
    "    Dense(6),Activation('sigmoid'),\n",
    "    Dense(kk),Activation('softmax'),\n",
    "])\n",
    "model.compile(optimizer='nadam', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "185151/185151 [==============================] - 14s 75us/step - loss: 0.6636 - acc: 0.5934\n",
      "Epoch 2/20\n",
      "185151/185151 [==============================] - 10s 53us/step - loss: 0.6187 - acc: 0.6478\n",
      "Epoch 3/20\n",
      "185151/185151 [==============================] - 10s 52us/step - loss: 0.6098 - acc: 0.6554\n",
      "Epoch 4/20\n",
      "185151/185151 [==============================] - 10s 54us/step - loss: 0.6066 - acc: 0.6617\n",
      "Epoch 5/20\n",
      "185151/185151 [==============================] - 10s 54us/step - loss: 0.6038 - acc: 0.6669\n",
      "Epoch 6/20\n",
      "185151/185151 [==============================] - 10s 54us/step - loss: 0.6016 - acc: 0.6707\n",
      "Epoch 7/20\n",
      "185151/185151 [==============================] - 10s 55us/step - loss: 0.5999 - acc: 0.6726\n",
      "Epoch 8/20\n",
      "185151/185151 [==============================] - 10s 53us/step - loss: 0.5988 - acc: 0.6752\n",
      "Epoch 9/20\n",
      "185151/185151 [==============================] - 10s 53us/step - loss: 0.5977 - acc: 0.6770\n",
      "Epoch 10/20\n",
      "185151/185151 [==============================] - 10s 52us/step - loss: 0.5966 - acc: 0.6795\n",
      "Epoch 11/20\n",
      "185151/185151 [==============================] - 10s 55us/step - loss: 0.5955 - acc: 0.6817\n",
      "Epoch 12/20\n",
      "185151/185151 [==============================] - 11s 59us/step - loss: 0.5943 - acc: 0.6824\n",
      "Epoch 13/20\n",
      "185151/185151 [==============================] - 10s 53us/step - loss: 0.5931 - acc: 0.6844\n",
      "Epoch 14/20\n",
      "185151/185151 [==============================] - 11s 61us/step - loss: 0.5922 - acc: 0.6854\n",
      "Epoch 15/20\n",
      "185151/185151 [==============================] - 11s 62us/step - loss: 0.5914 - acc: 0.6864 0s - loss: 0.5919 - acc\n",
      "Epoch 16/20\n",
      "185151/185151 [==============================] - 11s 59us/step - loss: 0.5907 - acc: 0.6864\n",
      "Epoch 17/20\n",
      "185151/185151 [==============================] - 12s 62us/step - loss: 0.5903 - acc: 0.6878\n",
      "Epoch 18/20\n",
      "185151/185151 [==============================] - 15s 80us/step - loss: 0.5899 - acc: 0.6880\n",
      "Epoch 19/20\n",
      "185151/185151 [==============================] - 15s 81us/step - loss: 0.5894 - acc: 0.6880\n",
      "Epoch 20/20\n",
      "185151/185151 [==============================] - 13s 71us/step - loss: 0.5893 - acc: 0.6887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1104d9400>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the net\n",
    "model.fit(X,Y, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy = \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5858105772554442"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "xx =X\n",
    "yy = news_final['FBPopMean']\n",
    "yy = np.array(yy)\n",
    "xx\n",
    "from sklearn.model_selection import train_test_split\n",
    "xx_train, xx_test, yy_train, yy_test = train_test_split(xx, yy, test_size=0.25, random_state=0)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression(max_iter=1000)\n",
    "logisticRegr.fit(xx_train, yy_train)\n",
    "print(\"Logistic Regression Accuracy = \")\n",
    "logisticRegr.score(xx_test, yy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation Scores\n",
      "0.11476229466915897\n",
      "-0.12238836200963803\n",
      "-0.14821643706152787\n",
      "0.07493108993647242\n",
      "0.019834891146101388\n"
     ]
    }
   ],
   "source": [
    "# Correlation scores\n",
    "print(\"Correlation Scores\")\n",
    "print(news_final['FBPopMean'].corr(news_final['TopicLabels']))\n",
    "print(news_final['FBPopMean'].corr(news_final['HeadlineSentiment']))\n",
    "print(news_final['FBPopMean'].corr(news_final['TitleSentiment']))\n",
    "print(news_final['FBPopMean'].corr(news_final['HourOfDay']))\n",
    "print(news_final['FBPopMean'].corr(news_final['DayOfWeek']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
